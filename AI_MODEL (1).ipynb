{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f026e48ec81f4cf0b230e642f92fa7d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab52e150ffa74f03a289e38db84718e5",
              "IPY_MODEL_300f3fe574f34bd1b3e7ad50a7120a1e",
              "IPY_MODEL_5fd595929e514fb9b49655165206521a"
            ],
            "layout": "IPY_MODEL_31660e8b9ad8437caaf39c5e90030985"
          }
        },
        "ab52e150ffa74f03a289e38db84718e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_137e245028ae48568a7d7089a8cd6e86",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c6288848ad2d44a3b14c06fb92f49e1f",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "300f3fe574f34bd1b3e7ad50a7120a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1f676eaefb34ec887e8d5549ab9b894",
            "max": 178481432,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e16aeefa537479195e0c9a551b4b414",
            "value": 178481432
          }
        },
        "5fd595929e514fb9b49655165206521a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b919ddba30c5413d8d7ef4459d3fe973",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_83a3593be5344b4aa730f3c9b58988c4",
            "value": "â€‡178M/178Mâ€‡[00:02&lt;00:00,â€‡50.7MB/s]"
          }
        },
        "31660e8b9ad8437caaf39c5e90030985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "137e245028ae48568a7d7089a8cd6e86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6288848ad2d44a3b14c06fb92f49e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1f676eaefb34ec887e8d5549ab9b894": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e16aeefa537479195e0c9a551b4b414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b919ddba30c5413d8d7ef4459d3fe973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83a3593be5344b4aa730f3c9b58988c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zzG0B-DJ9G6",
        "outputId": "8fed0d64-5eb5-40ce-bc00-79607ffb0e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision scikit-learn opencv-python tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3LYy6YkOyeT",
        "outputId": "46826093-f1df-4b76-fb4c-a2feadbf4c1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/New folder.zip\"\n",
        "if os.path.exists(file_path):\n",
        "    print(\"âœ… ZIP file mil gayi:\", file_path)\n",
        "else:\n",
        "    print(\"âŒ ZIP file nahi mili:\", file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqrGsdaBPV6-",
        "outputId": "2cef6a06-ab3a-4c5b-dc13-3324c71b7792"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ZIP file mil gayi: /content/drive/MyDrive/New folder.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/New folder.zip\" -d \"/content/sample_data\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6s6iIJ6Sr9C",
        "outputId": "1dcf1951-2627-40d4-9b5a-2b2863c30b13"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/New folder.zip\n",
            "  inflating: /content/sample_data/New folder/IMG_0219.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0220.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0221.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0222.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0223.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0301.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0302.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0303.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0304.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0305.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0310.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0311.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0312.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0313.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0314.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0315.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0316.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0317.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0318.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0319.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0320.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0321.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0324.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0325.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0326.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_0327.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7446.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7447.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7448.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7451.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7452.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7453.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7455.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7456.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7457.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7458.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7459.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7460.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7461.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7462.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7463.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7467.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7471.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7472.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7474.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7477.JPG  \n",
            "  inflating: /content/sample_data/New folder/IMG_7478.JPG  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.eval()\n",
        "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IvHCaxzN1st",
        "outputId": "529b613f-9618-4b45-cd09-1bf061ec18e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:01<00:00, 97.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return transform(img).unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "iL-3JVwhN83w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_feature_vector(img_path):\n",
        "    img_tensor = preprocess_image(img_path)\n",
        "    with torch.no_grad():\n",
        "        features = feature_extractor(img_tensor).squeeze().numpy()\n",
        "    return features / np.linalg.norm(features)\n"
      ],
      "metadata": {
        "id": "HJYyJeWTOEmN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def pair_images(image_paths, features):\n",
        "    pairs = []\n",
        "    used = set()\n",
        "    for i, img1 in enumerate(image_paths):\n",
        "        if img1 in used:\n",
        "            continue\n",
        "        best_match = None\n",
        "        best_score = 0\n",
        "        for j, img2 in enumerate(image_paths):\n",
        "            if img1 == img2 or img2 in used:\n",
        "                continue\n",
        "            sim = cosine_similarity([features[img1]], [features[img2]])[0][0]\n",
        "            if sim > best_score:\n",
        "                best_score = sim\n",
        "                best_match = img2\n",
        "        if best_match:\n",
        "            pairs.append((img1, best_match))\n",
        "            used.add(img1)\n",
        "            used.add(best_match)\n",
        "    return pairs\n"
      ],
      "metadata": {
        "id": "7fUm4fG_OIxH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_room(image_path):\n",
        "    name = image_path.lower()\n",
        "    if \"kitchen\" in name:\n",
        "        return \"Kitchen\"\n",
        "    elif \"living\" in name:\n",
        "        return \"Living Room\"\n",
        "    elif \"closet\" in name:\n",
        "        return \"Closet\"\n",
        "    else:\n",
        "        return \"Room\"\n"
      ],
      "metadata": {
        "id": "d9THUJ41OWab"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def save_pairs(pairs, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    for i, (before, after) in enumerate(pairs):\n",
        "        label = classify_room(before + after)\n",
        "        room_folder = os.path.join(output_folder, label)\n",
        "        os.makedirs(room_folder, exist_ok=True)\n",
        "        shutil.copy(before, os.path.join(room_folder, f\"{label.lower()}_{i}_before.jpg\"))\n",
        "        shutil.copy(after, os.path.join(room_folder, f\"{label.lower()}_{i}_after.jpg\"))\n"
      ],
      "metadata": {
        "id": "dIfWiOLNOazY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def organize_images(input_folder, output_folder):\n",
        "    image_paths = []\n",
        "    for root, dirs, files in os.walk(input_folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"âœ… Found {len(image_paths)} images.\")\n",
        "    print(\"ğŸ“¸ First few images:\", image_paths[:5])\n",
        "\n",
        "    features = {img: get_feature_vector(img) for img in tqdm(image_paths, desc=\"Extracting Features\")}\n",
        "    pairs = pair_images(image_paths, features)\n",
        "    save_pairs(pairs, output_folder)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1d5gxN4LOenW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organize_images(\"/content/sample_data/New folder\", \"/content/Organized_Project\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNBUKrZXb1zp",
        "outputId": "738fff3c-9d66-4bd9-e9a3-d190b465d04b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Found 47 images.\n",
            "ğŸ“¸ First few images: ['/content/sample_data/New folder/IMG_7456.JPG', '/content/sample_data/New folder/IMG_0311.JPG', '/content/sample_data/New folder/IMG_7461.JPG', '/content/sample_data/New folder/IMG_0221.JPG', '/content/sample_data/New folder/IMG_0326.JPG']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:12<00:00,  3.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm --quiet\n"
      ],
      "metadata": {
        "id": "fhqvv0-yc2ZZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "import json\n",
        "import urllib.request\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = timm.create_model('convnext_tiny.fb_in22k', pretrained=True)\n",
        "model.eval().to(device)\n",
        "\n",
        "# Load class index labels\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json'\n",
        "with urllib.request.urlopen(url) as f:\n",
        "    idx_to_labels = json.load(f)\n",
        "\n",
        "ROOM_LABELS = {\n",
        "    \"kitchen\": [\"kitchen\", \"oven\", \"stove\", \"microwave\"],\n",
        "    \"living room\": [\"living\", \"tv\", \"sofa\", \"couch\"],\n",
        "    \"closet\": [\"closet\", \"wardrobe\", \"clothes\"],\n",
        "    \"bathroom\": [\"bathroom\", \"toilet\", \"sink\", \"shower\"],\n",
        "    \"bedroom\": [\"bed\", \"pillow\"],\n",
        "}\n",
        "\n",
        "def predict_room_type(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        img_tensor = transform(image).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(img_tensor)\n",
        "        _, predicted = outputs.max(1)\n",
        "        label_text = idx_to_labels[str(predicted.item())][1].lower()\n",
        "        for room, keywords in ROOM_LABELS.items():\n",
        "            if any(keyword in label_text for keyword in keywords):\n",
        "                return room.capitalize()\n",
        "        return \"Other\"\n",
        "    except:\n",
        "        return \"Other\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "f026e48ec81f4cf0b230e642f92fa7d6",
            "ab52e150ffa74f03a289e38db84718e5",
            "300f3fe574f34bd1b3e7ad50a7120a1e",
            "5fd595929e514fb9b49655165206521a",
            "31660e8b9ad8437caaf39c5e90030985",
            "137e245028ae48568a7d7089a8cd6e86",
            "c6288848ad2d44a3b14c06fb92f49e1f",
            "c1f676eaefb34ec887e8d5549ab9b894",
            "9e16aeefa537479195e0c9a551b4b414",
            "b919ddba30c5413d8d7ef4459d3fe973",
            "83a3593be5344b4aa730f3c9b58988c4"
          ]
        },
        "id": "iXLL1OKbc6XW",
        "outputId": "bc5c011f-8f21-447e-9fba-fc3c4d6fe378"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/178M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f026e48ec81f4cf0b230e642f92fa7d6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pairs(pairs, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    for i, (before, after) in enumerate(pairs):\n",
        "        room_type = predict_room_type(before)\n",
        "        room_folder = os.path.join(output_folder, room_type)\n",
        "        os.makedirs(room_folder, exist_ok=True)\n",
        "        shutil.copy(before, os.path.join(room_folder, f\"{room_type.lower()}_{i}_before.jpg\"))\n",
        "        shutil.copy(after, os.path.join(room_folder, f\"{room_type.lower()}_{i}_after.jpg\"))\n"
      ],
      "metadata": {
        "id": "f2hKoCW1dCoc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organize_images(\"/content/sample_data/New folder\", \"/content/Organized_Project\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqcT9ZnFdIVw",
        "outputId": "47d6b516-d6bf-41e2-aa4f-131ae49bdd88"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Found 47 images.\n",
            "ğŸ“¸ First few images: ['/content/sample_data/New folder/IMG_7456.JPG', '/content/sample_data/New folder/IMG_0311.JPG', '/content/sample_data/New folder/IMG_7461.JPG', '/content/sample_data/New folder/IMG_0221.JPG', '/content/sample_data/New folder/IMG_0326.JPG']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:11<00:00,  3.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "\n",
        "# Load a pre-trained model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet18(pretrained=True)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Dummy room labels â€“ You can improve this by fine-tuning\n",
        "room_keywords = {\n",
        "    \"kitchen\": [\"kitchen\", \"microwave\", \"oven\", \"refrigerator\"],\n",
        "    \"bedroom\": [\"bed\", \"pillow\", \"blanket\"],\n",
        "    \"bathroom\": [\"toilet\", \"sink\", \"bath\"],\n",
        "    \"living_room\": [\"sofa\", \"television\", \"lamp\"],\n",
        "    \"other\": []\n",
        "}\n",
        "\n",
        "def classify_room_type(image_path):\n",
        "    input_image = Image.open(image_path).convert(\"RGB\")\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    input_tensor = preprocess(input_image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Get class label from ImageNet\n",
        "    class_idx = predicted.item()\n",
        "    label = models.detection.IMAGE_NET_CLASSES[class_idx].lower()\n",
        "\n",
        "    for room, keywords in room_keywords.items():\n",
        "        if any(keyword in label for keyword in keywords):\n",
        "            return room\n",
        "    return \"other\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zslcuZ8cd8Qy",
        "outputId": "3468e7ba-b343-4ca7-a5a7-5947ef20a6e4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 67.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def guess_room_type(image_path):\n",
        "    filename = os.path.basename(image_path).lower()\n",
        "\n",
        "    if \"kitchen\" in filename or \"031\" in filename:\n",
        "        return \"Kitchen\"\n",
        "    elif \"living\" in filename or \"032\" in filename:\n",
        "        return \"Living Room\"\n",
        "    elif \"bed\" in filename or \"745\" in filename:\n",
        "        return \"Bedroom\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "def save_pairs_by_room_type(pairs, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for before_path, after_path in pairs:\n",
        "        room_type = guess_room_type(before_path)\n",
        "        room_folder = os.path.join(output_folder, room_type)\n",
        "        os.makedirs(room_folder, exist_ok=True)\n",
        "\n",
        "        # Copy files with proper naming\n",
        "        shutil.copy2(before_path, os.path.join(room_folder, f\"before_{os.path.basename(before_path)}\"))\n",
        "        shutil.copy2(after_path, os.path.join(room_folder, f\"after_{os.path.basename(after_path)}\"))\n",
        "\n",
        "    print(\"âœ… Images saved by room type.\")\n",
        "\n",
        "# âœ… Call this after pairing\n",
        "save_pairs_by_room_type(pairs, \"/content/Organized_Project\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxUxfo7Oe2s8",
        "outputId": "dcf7103b-dfdd-4974-d117-1df438950b06"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Images saved by room type.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "output_folder = \"/content/Organized_Project\"\n",
        "\n",
        "# Dictionary to hold final result\n",
        "organized_data = {}\n",
        "\n",
        "# Loop through subfolders like Kitchen, Living Room, etc.\n",
        "for room_type in os.listdir(output_folder):\n",
        "    room_path = os.path.join(output_folder, room_type)\n",
        "    if os.path.isdir(room_path):\n",
        "        files = sorted(os.listdir(room_path))\n",
        "        before_img = [f for f in files if 'before' in f.lower()]\n",
        "        after_img = [f for f in files if 'after' in f.lower()]\n",
        "\n",
        "        # Pair up images and store\n",
        "        pairs = list(zip(before_img, after_img))\n",
        "        organized_data[room_type] = []\n",
        "\n",
        "        for b, a in pairs:\n",
        "            organized_data[room_type].append({\n",
        "                \"before\": b,\n",
        "                \"after\": a,\n",
        "                \"title\": room_type\n",
        "            })\n",
        "\n",
        "# Save the JSON file\n",
        "json_path = os.path.join(output_folder, \"image_pairs_by_room.json\")\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(organized_data, f, indent=4)\n",
        "\n",
        "print(f\"âœ… JSON saved to: {json_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHqLfgeMfqhJ",
        "outputId": "e16559ae-cbb0-4aff-a2fd-9237a792f989"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… JSON saved to: /content/Organized_Project/image_pairs_by_room.json\n"
          ]
        }
      ]
    }
  ]
}